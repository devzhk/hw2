\documentclass[11pt]{article}
\setlength{\textwidth6.5in} \setlength{\textheight8.8in}
\setlength{\oddsidemargin0in} \setlength{\topmargin-0.3in}
\setlength{\unitlength}{1cm}

\def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\newcommand{\qedbox}{\vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}}
\newcommand{\qed}{\nopagebreak\mbox{}\hfill\qedbox\bigskip}
\newcommand{\comment}[1]{}
%\def\solution{\textbf{Solution: }}
\newcommand{\solution}[1]{{\ \\ \noindent\bf Solution: } #1}

% whether this is for a solution or not
\newif\ifsol
\solfalse

\ifsol
\def\includesolution{\input}
\else
\def\includesolution{\comment}
\fi

\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{graphics, graphicx, subcaption}
%\usepackage[english]{babel}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{framed}
\usepackage{enumerate}

\usepackage{hyperref}

\newcommand{\ra}{\rightarrow}
\newcommand{\ua}{\uparrow}
\newcommand{\prob}[1]{P\left(#1\right)}
\newcommand{\imp}{\Rightarrow}
\newcommand{\re}{\mathbb{R}}
\newcommand{\Exp}[1]{\mathbb{E}\left[#1\right]} %Expectation


\usepackage{graphicx, subcaption}

\usepackage{listings}
\usepackage{color}

\lstset{language=Python}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstset{
    frame=single,
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    numbers=left,
    numbersep=5pt,
    keepspaces=true,                 
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    basicstyle=\ttfamily\footnotesize,
}



\begin{document}

%-----------------This is the header/topics portion--------------------
\noindent \rule{\textwidth}{.5mm} \noindent \makebox[1.5in][l]
{\bf CMS/CS/EE 144}  \hfill  \makebox[1.5in][r]{\bf Gurus: Myra \& Yiheng}

\noindent \centerline{\bf {\Large Homework 2: Starting with a crawl}}

\noindent \makebox[1.5in][l]{Assigned: 01/19/22}  \hfill
\makebox[1.5in][r]{Due: 01/25/22 1:00pm PT} \noindent
\rule{\textwidth}{.5mm}

%-----------------This is the header/topics portion--------------------

\noindent \textit{Collaboration is allowed for all problems and at the top of your homework sheet, please list all the people
with whom you discussed. Crediting help from other classmates will not take away any credit from you.
The details of the collaboration policy for this course are available in the Resources tab on Piazza.\\}

\noindent\textit{You must turn in your homework electronically via Gradescope. \textbf{Be sure to submit your homework as a single file.} }\\

\noindent\textit{Start early and come to office hours with your questions! We also encourage you to post your questions on Piazza, as well as answer the questions asked by others on Piazza.}

\section{Before you can walk you must... [35 points]}
\label{prob:crawler}


A web crawler is a program that automatically traverses and collects
the web pages on the Internet.  Crawlers are widely used by search
engines to find and retrieve what's on the web so that they can build
an index of the pages for searching. However, they're also useful for
many other reasons, e.g., for gathering data, for validating the links
on a web page, or for gathering everything from a particular site. You
are going to implement a very primitive web crawler to explore the web
pages in the Caltech domain (URLs containing ``.caltech.edu'') and
count the number of hyperlinks each page contains.

 {\bf The basic idea of crawling:} A web crawler starts by
retrieving a web page, parsing the HTML source and extracting all
hyperlinks in it.  Then, the crawler uses these hyperlinks to retrieve
those pages and extracts the hyperlinks in them. This process repeats
until the crawler has followed all links and downloaded all
pages. Therefore, a crawler requests web pages just as a web browser
does, but it browses automatically, following all the the hyperlinks
in each page. Although the idea is simple, there are a few issues that
come up:
\begin{enumerate}
\item Many web pages contain links to multimedia/data files, the
  crawler should not waste bandwidth downloading these files.
\item Many web sites deliver dynamic content; the web page is
  generated dynamically based on a query string specified in the
  URL. A crawler can get trapped on such a site, since there are
  potentially `infinitely many' pages. For example, on an online
  calendar, the crawler can keep following the links of dates/months
  and get trapped in an endless loop.
\item Given that the crawler has extracted a long queue of links to
  visit, which one should be selected to visit next?
\item Given that the crawler has seen a page already, when should it
  go back to revisit the page and check for new and changed links?
  (You can ignore this for this assignment.)
\end{enumerate}
There are many other issues too, such as the robot exclusion standard,
Javascript in web pages, ill-formed HTML and so on.

{\bf Your task:}  Your task is to write a crawler
that, given a URL in the Caltech domain, retrieves the web page,
extracts all hyperlinks in it, counts the number of hyperlinks,
follows the extracted hyperlinks to retrieve more pages in Caltech
domain, and then repeats the process for each successive page. During
this process you must keep updating the number of hyperlinks on a web
page and the number of hyperlinks which point to that page.

\textbf{We highly recommend writing this crawler in Python.} We are giving you Python code to aid in retrieving and parsing the web pages. We cannot guarantee that the TAs will be able to help you with programming issues if you do not use Python 2.7 or Python 3.

You may write your crawler in any programming language you prefer, but
you need to turn in a program that the TAs can test on the Caltech CS
cluster. You should feel free to use third party libraries, but you must specify which ones to include and how to run your code. If you find you are having trouble with these parts of the problem, contact the TAs, since this is not meant to be the focus of the problem.

Instead, your main task is to code the traversal of the web graph.
Specifically, you should design your own selection policy.  (You can
ignore the revisit policy for this assignment and visit each page only
once.) You could do this using something similar to a
breadth-first-search or depth-first-search, or something more
sophisticated.  A key component in this will be to ensure that you do
not visit pages more than once and that you do not get trapped.  (To
prevent your web crawler from getting trapped you will have to deal
with issues related to ``Non-HTML pages'' and ``Dynamic pages.'') \\
You should also save your network, as it will be necessary in problem 4c. 

\noindent\textbf{Important notes:}

\begin{itemize}
\item If you search the web, you will likely be able to find source
code for a crawler.  We expect that you will \emph{not} look at
external source code when doing this assignment and that you will
write your own crawler.

\item Ensure that your crawler stays within the Caltech domain.
  Do not crawl library linked services (e.g. JSTOR, IEEE Xplore,
  Wiley Online Library, etc.) or systematically download academic
  journal articles. These are violations of the terms of service.

\item An efficient crawler can easily be mistaken for a malicious denial of service (DoS) attack. Do not do DoS attacks on the Caltech servers! You should limit the number of parallel requests and/or the time
  between requests. For a discussion on web crawling etiquette, see\\
  http://en.wikipedia.org/wiki/Web\_crawler\#Politeness\_policy.

\item You should work on this problem individually. Feel free to discuss ideas with your classmates, but you should not look at any other students' code and you should write your code by yourself.
\end{itemize}

\noindent{\bf What you turn in:} You should submit the following after
crawling at least 2000 HTML pages in the Caltech domain starting from
www.caltech.edu. Note that if a crawled page has URLs that were never actually visited by the crawler, these URLs should not be included in the network.
\begin{enumerate}
\item Your code, including libraries used and how to run it.
\item An explanation of the selection policy you chose and its
  strengths and weaknesses.
\item Two histograms. One for the number of hyperlinks per page.  One for
  the number of hyperlinks which point to each page.
\item Two complementary cumulative distribution functions (ccdf).  One
  for the number of hyperlinks per page. One for the number of
  hyperlinks which point to each page.
\item The average clustering coefficient and the overall clustering coefficient of the graph. Treat the edges as undirected for these calculations.
\item The average and maximal diameter of the graph. Treat the edges as undirected for these calculations.
\item A comparison of the degree distribution, the clustering
coefficients, and the diameters with those in Problem 2. Can you describe the similarities and differences regarding the "universal" properties between collaboration network and the web graph?

If you calculated the maximal and average diameter using a different algorithm from that used in Problem 2, be sure to describe the algorithm that you used here.
\end{enumerate}
Submit (1) as part of your HW on Gradescope.\\

\noindent{\bf Helpful tips:} Python scripts (\texttt{fetcher3.py} and \texttt{fetcher273.py}) to extract all hyperlinks
in a web page specified by an URL are provided in the zip file for this homework. It is a very simple program based only on the Python standard libraries, so you just can install Python (http://www.python.org) and run it.  The
code is about 100 lines long, including lots of comments. Take a look
at it even if you are not familiar with Python. The script performs the
following tasks.
\begin{enumerate}
\item Use the URL to make a connection and fetch the http header. If
  the Content-Type is not ``text/html'', return ``None''.
\item Retrieve the web page content.
\item Use an html parser to parse the page and extract all hyperlinks.
\item Refine the hyperlinks, e.g., convert relative URLs to absolute
  URLs, and ignore the parameters of dynamic pages.
\end{enumerate}
If you use Python, you can import this file and simply call the
function ``fetch\_links(URL)'' to get a list (may be empty) which
contains all hyperlinks in the web page specified by the URL. It
returns ``None'' if the URL does not point to a valid HTML page or if
some error occurred. If you use another programming language, you can
still use this script by making a system call to execute it and piping
the output to your program. The output is a list (may be empty "[]")
of hyperlinks which looks like: "[`http://today.caltech.edu',
`http://www.caltech.edu/copyright/']". The output is ``None'' if the
URL does not point to a valid HTML page or if some error
occurred. You should catch (except) any other errors that occur.

If you use Python, we highly recommend using the package \texttt{networkx} (available in both Python 2.7 and 3) to do some of the network computation and \texttt{pickle} to save the network. You'll also want to use this package for Problem 4.

As a reference point, our solution code in Python is about 100 lines long, including comments.


\section{Working with real data [20 points]}
\label{prob:realdata}

We'll now look for first hand evidence of the `universal properties' by studying a real world dataset. We'll work with a collaboration network between researchers working in the area of General Relativity and Quantum Cosmology. If an author $i$ co-authored a paper with author $j$, the graph contains a undirected edge from $i$ to $j$. If the paper is co-authored by $k$ authors this generates a completely connected (sub)graph on $k$ nodes. The original dataset contains 5242 nodes, but we'll work with the largest connected component of this network, consisting of 4158 nodes. You can download the text file from the file called gr\_qc\_coauthorships.txt on piazza under the resources tab, each line of which contains an edge in the network.


\begin{enumerate}[(a)]
\item Plot the histogram, as well as a complementary cumulative distribution function (ccdf) of the node degrees. Compute the average clustering coefficient, the overall clustering coefficient, the maximal diameter, and the average diameter.
\item Calculate the number of triangles $T$. Assuming $T = \Exp T$, the expected number of triangles of a Erd\H{o}s--R\'{e}nyi graph with the same number of nodes (4158), calculate the parameter $p$ of the Erd\H{o}s--R\'{e}nyi graph $G(n, p)$.
\item What distribution should the node degrees of a Erd\H{o}s--R\'{e}nyi graph take on? Is the Erd\H{o}s--R\'{e}nyi model a good model for this graph (use the histogram from part a). Do we need the histogram to conclude this? Think of where the data came from.

\end{enumerate}

{\bf Note:} It is okay (recommended!) to use existing network libraries (e.g., networkx for python) to do the calculations.  If you do this, please specify the library and the functions you are using.


\section{{Getting to know \em Erd\H{o}s--R\'{e}nyi} [30 points]}


In class, we've talked a bit about the {\em Erd\H{o}s--R\'{e}nyi}
random graph model and begun to explore how well it models the four
``universal'' properties that we've been focusing on.  In this
problem, you will revisit three of these four properties and prove
some additional results.

Recall, the {\em Erd\H{o}s--R\'{e}nyi}
random graph model creates an undirected random graph, denoted by
$G(n,p),$ by considering $n$ vertices and letting every possible
edge between vertices occur independently with probability $p$.

Since we already showed in class that the degree of any vertex in a $G(n, p)$ is binomially distributed, in this problem we will focus on clustering and diameter.

\begin{enumerate}[(a)]



\item {\bf Clustering:}
    \begin{enumerate}[(i)]
        \item Calculate the expected number of triangles, denoted $E[T]$, that $G(n,p)$ contains.

        Prove that $E[T]$ has a threshold.
        Specifically, find a function $\pi(n)$ such that $\lim_{n \ra \infty} \Exp{T} = \infty$ if $p \in \omega(\pi(n))$ and $\lim_{n \ra \infty} \Exp{T} = 0$ if $p \in o(\pi(n))$?\footnote{
            In this problem, all functions are with respect to $n$ unless otherwise stated.
            For a reminder on the notation, please consult \url{https://en.wikipedia.org/wiki/Big\_O\_notation\#Family\_of\_Bachmann\%E2\%80\%93Landau\_notations}
        }

        \item Let $X$ be the event that a triangle is contained in $G$.
            Note that the previous part is insufficient to show that $\pi(n)$ is a threshold for $X$ (not for credit, but think about why!).
            But we can show this fact with second moment method -- the same way we proved the result for isolated vertices in class. Since the general case is a bit nasty, we will focus on a specific setting here: $p(n) = \pi(n)\log(n)$.
            \begin{itemize}
                \item Show that $\text{Var}(T) \in \Theta(\log^3(n))$. Hint: First think of how to express $T$ as sum of 0-1 random variables.  Then, use case-work to understand the covariance terms that result.
                \item Use Chebyshev's Inequality to show that $\Pr(T = 0) \in o(1)$.
                    Conclude that $\lim_{n\to\infty} \Pr(X) = 1$ for this particular $p$.
            \end{itemize}
            \textbf{Extra Credit (5 points):} Briefly argue how to generalize this argument for any $p \in \omega(\pi(n))$, you do not have to give complete proof for this part.
    \end{enumerate}


\item {\bf Diameter:} Suppose $p \in (0,1)$ is held constant. Prove
  that the (maximal) diameter of $G(n,p)$ equals two with a probability that
  approaches 1 as $n$ becomes large, i.e., prove that
  $$\lim_{n \ra \infty} \prob{diameter(G(n,p)) = 2} = 1.$$

\end{enumerate}

\section{Visualize your network [15 points]}

In this problem, we will familiarize ourselves with the importance and challenges of visualizing networks. More specifically, we will be visualizing the {\em Erd\H{o}s--R\'{e}nyi} model, which we've seen in class and Problem 3, the symmetric stochastic block model (a generalization of the {\em Erd\H{o}s--R\'{e}nyi} model we will study later in the class), and the web graph that we generated in Problem 1. 

Before starting your task, we'll first introduce the stochastic block model, which is an important tool when studying clustering algorithms.  The \textit{stochastic block model (SBM)} is a random graph model with planted clusters.

The model $SBM(n, p, W)$ defines an $n$-vertex random graph with labeled vertices for positive integers $n$, $k$, a probability vector $p$ of dimension $k$, and a symmetric matrix $W$ of dimension $k \times k$ with entries $W_{i,j} \in [0, 1]$. Each vertex is assigned a community label in $\{1,..., k\}$ independently under the community prior $p$, and pairs of vertices with community labels $i$ and $j$ connect independently with probability $W_{i,j}$. The SBM is called \emph{symmetric} if $p$ is uniform and if $W$ takes the same value $A$ on the diagonal and the same value $B$ outside the diagonal. We can represent the symmetric SBM as $SSBM(n, k, A, B)$. \\
\textbf{Note:} For this problem, you only need to consider simple graphs with no self-loops or parallel edges.
\vspace{.1in}
 
\noindent\textbf{Your task}: Visualize the following three networks:

\begin{enumerate}[(a)]
    \item \textbf{Erd\H{o}s--R\'{e}nyi (4 points)}: Sample a network from the {\em Erd\H{o}s--R\'{e}nyi} model with $n = 30$ vertices where every possible edge between vertices occurs independently with probability $p = 0.2$. 
    
    \item \textbf{Symmetric Stochastic Block (4 points)}: Sample a network from the symmetric stochastic block model with $n = 30$ vertices, $k = 3$ communities, $A = 0.7$, and $B = 0.1$.
    
    \item \textbf{The web (7 points)}: Create two networks using the first $n$ nodes crawled by the web crawler that you created in Problem 1. One with $n = 100$ nodes and one with $n = 300$ nodes. 
\end{enumerate}

\noindent Specifically, for each network, you should:

 \begin{enumerate}[(i)]
     \item Write a function that can generate a network for both $G(n,p)$ and $SSBM(n, k, A, B)$. Please avoid using direct functions (e.g., networkx.generators.random\_graphs.erdos\_renyi\_graph). Include your code as part of your pdf submission.
     
     \item Visualize the network. You may use existing network libraries (e.g., networkx for Python). Play around with the visual presentations of the networks, such as layout (e.g., random, radial, spring), colors, node sizes, edge thickness, etc. Consider the most effective way to convey the information presented by this type of network. Present two visualizations that provide contrasting perspectives on the network. 
     
     Note: In addition to networkx.drawing.layout, Graphviz (networkx.drawing.nx\_pydot.graphviz\_layout) also has some useful layouts to explore. 
     
     \item Provide analysis of the visualization. Discuss the effectiveness of the different forms of network visualizations you explored, including whether different visualizations were more effective for the different networks.  Explain any advantages or drawbacks of the approaches that you experimented with.  
     
 \end{enumerate}

\end{document}
